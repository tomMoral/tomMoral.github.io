
<html lang="en-US">
<!--<![endif]-->
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width" />
<title>Thomas Moreau</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<!--[if lt IE 9]>
<script src="http://dpkingma.com/wordpress/wp-content/themes/twentytwelve/js/html5.js" type="text/javascript"></script>
<![endif]-->
<script src="https://use.fontawesome.com/ae904b9937.js"></script>



<link rel="shortcut icon" href="/images/favicon.png">
<link rel="stylesheet" type="text/css" href="/css/style.css" />
<link rel="stylesheet" href="/css/academicons.css"/>
<script type="text/javascript" src="/javascript/lib.js"></script>
<!-- highlight current page in navigation -->
<script>
$(function(){
  $('a').each(function() {
    if ($(this).prop('href') == window.location.href) {
      $(this).parent().addClass('current-menu-item');
    }
  });
});
</script>

</head>

<body class="home">
<div id="sidebar">
    <header id="masthead" class="site-header" role="banner">
        <hgroup>
            <img class="profile_img" src="https://s.gravatar.com/avatar/1c7c7939d1173cd6f455ce4313e831c5?s=150&r=x" />
            <div class="info">
                <h2 class="site-title"><a href="/about">Thomas Moreau</a></h2>
                <h4 class="site-description">Parietal - Inria Saclay</h4>
                <h5 class="email"> thomas.moreau [AT] inria.fr</h5>
            </div>
        </hgroup>

        <nav id="site-navigation" class="main-navigation" role="navigation">
            <div class="menu-menu-1-container"><ul id="menu-nav" class="menu-nav">
                <li class="nav-menu-item"><a href="/about">About</a></li>
                <li class="nav-menu-item"><a href="/publications">Publications</a></li>
                <li class="nav-menu-item"><a href="/talks">Talks and Videos</a></li>
                <li class="nav-menu-item"><a href="/oss">Open Source Software</a></li>
            </ul></div>
        </nav><!-- #site-navigation -->
        <div id="social-media">
            <a href="https://github.com/tomMoral">
                <i class="fa fa-github fa-3x"></i></a>
            <a href="https://scholar.google.fr/citations?user=HEO_PsAAAAAJ">
                <i class="ai ai-google-scholar ai-3x"></i></a>
            <a href="https://stackoverflow.com/story/tomMoral">
                <i class="fa fa-stack-overflow fa-3x"></i></a>
            <a href="https://linkedin.com/in/tomMoral">
                <i class="fa fa-linkedin fa-3x"></i></a>
        </div>
    </header><!-- #masthead -->
</div>
</div>

<div id="page">
    <div class="page-content">
    <article class="inner-text">
	<header class="entry-header">
		<h1 class="entry-title">Publications</h1>
	</header>

	<div class="entry-content">


    
    <div class="publication">
        <div class="project_item">
            <span class="title">Super-efficiency of automatic differentiation for functions defined as a minimum </span>
            <span style="width:2em;"></span>

            <a href="https://arxiv.org/pdf/2002.03722">
                <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
            </a>



            <br>
            <span class="authors">Pierre Ablin; Gabriel Peyré and Thomas Moreau,</span>
            <span class="date">Feb 2020,</span>

                <span><i>preprint /</i></span>


        </div>

        <input type="checkbox" class="btnCtrl" id="pub15"/>
        <label class="btn display-status" for="pub15">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                We study the different techniques to differentiate a function defined as a min of an other.
            </div>
            <div class="details">
                In min-min optimization or max-min optimization, one has to compute the gradient of a function defined as a minimum. In most cases, the minimum has no closed-form, and an approximation is obtained via an iterative algorithm. There are two usual ways of estimating the gradient of the function: using either an analytic formula obtained by assuming exactness of the approximation, or automatic differentiation through the algorithm. In this paper, we study the asymptotic error made by these estimators as a function of the optimization error. We find that the error of the automatic estimator is close to the square of the error of the analytic estimator, reflecting a super-efficiency phenomenon. The convergence of the automatic estimator greatly depends on the convergence of the Jacobian of the algorithm. We analyze it for gradient descent and stochastic gradient descent and derive convergence rates for the estimators in these cases. Our analysis is backed by numerical experiments on toy problems and on Wasserstein barycenter computation. Finally, we discuss the computational complexity of these estimators and give practical guidelines to chose between them.
            </div>
        </div>
    </div>

    
    <div class="publication">
        <div class="project_item">
            <span class="title">Learning step sizes for unfolded sparse coding </span>
            <span style="width:2em;"></span>

            <a href="https://hal.archives-ouvertes.fr/hal-02140383v1">
                <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
            </a>


            <a href="https://github.com/tomMoral/adopty">
                <i class="fa fa-github-alt" aria-hidden="true"></i>
            </a>


            <a href="/talks/19_12_12_poster_slista.pdf">
                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>

            <br>
            <span class="authors">Pierre Ablin; Thomas Moreau; Mathurin Massias; Alexandre Gramfort,</span>
            <span class="date">Dec 2019,</span>

                <span class="conference">In proceedings of <i>Advances in Neural Information Processing Sytems (NeurIPS)</i></span>


        </div>

        <input type="checkbox" class="btnCtrl" id="pub13"/>
        <label class="btn display-status" for="pub13">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                This paper presents a theoretical study on how LISTA can learn to accelerate computation compared to ISTA based on larger step sizes adapted to the sparsity distribution of the solution estimate. this mechanism is the only one which ensure assymptotic convergence to the Lasso estimator.
            </div>
            <div class="details">
                Sparse coding is typically solved by iterative optimization techniques, such as the Iterative Shrinkage-Thresholding Algorithm (ISTA). Unfolding and learning weights of ISTA using neural networks is a practical way to accelerate estimation. In this paper, we study the selection of adapted step sizes for ISTA. We show that a simple step size strategy can improve the convergence rate of ISTA by leveraging the sparsity of the iterates. However, it is impractical in most large-scale applications. Therefore, we propose a network architecture where only the step sizes of ISTA are learned. We demonstrate that for a large class of unfolded algorithms, if the algorithm converges to the solution of the Lasso, its last layers correspond to ISTA with learned step sizes. Experiments show that our method is competitive with state-of-the-art networks when the solutions are sparse enough.
            </div>
        </div>
    </div>

    
    <div class="publication">
        <div class="project_item">
            <span class="title">A Data Set for the Study of Human Locomotion with Inertial Measurements Units </span>
            <span style="width:2em;"></span>

            <a href="http://www.ipol.im/pub/art/2019/265/">
                <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
            </a>


            <a href="https://github.com/deepcharles/gait-data">
                <i class="fa fa-github-alt" aria-hidden="true"></i>
            </a>


            <br>
            <span class="authors">Charles Truong, Rémi Barrois-Müller, Thomas Moreau, Clément Provost, Aliénor Vienne-Jumeau, Albane Moreau, Pierre-Paul Vidal, Nicolas Vayatis, Stéphane Buffat, Alain Yelnik, Damien Ricard, Laurent Oudre,</span>
            <span class="date">Nov 2019,</span>

                <span><i>Image Processing On Line</i></span>


        </div>

        <input type="checkbox" class="btnCtrl" id="pub14"/>
        <label class="btn display-status" for="pub14">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                A data set of 1020 multivariate gait signals collected with two inertial measurement units, from 230 subjects undergoing a fixed protocol.
            </div>
            <div class="details">
                This article thoroughly describes a data set of 1020 multivariate gait signals collected with two inertial measurement units, from 230 subjects undergoing a fixed protocol: standing still, walking 10 m, turning around, walking back and stopping. In total, 8.5~h of gait time series are distributed. The measured population was composed of healthy subjects as well as patients with neurological or orthopedic disorders. An outstanding feature of this data set is the amount of signal metadata that are provided. In particular, the start and end time stamps of more than 40,000 footsteps are available, as well as a number of contextual information about each trial. This exact data set was used in [Oudre et al., Template-based step detection with inertial measurement units, Sensors 18, 2018] to design and evaluate a step detection procedure.
            </div>
        </div>
    </div>

    
    <div class="publication">
        <div class="project_item">
            <span class="title">Distributed Convolutional Dictionary Learning (DiCoDiLe): Pattern Discovery in Large Images and Signals </span>
            <span style="width:2em;"></span>

            <a href="/publications/main_dicodile_with_supp.pdf">
                <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
            </a>


            <a href="https://github.com/tomMoral/dicodile">
                <i class="fa fa-github-alt" aria-hidden="true"></i>
            </a>


            <br>
            <span class="authors">Thomas Moreau and Alexandre Gramfort,</span>
            <span class="date">Nov 2019,</span>

                <span><i>preprint /</i></span>


        </div>

        <input type="checkbox" class="btnCtrl" id="pub9"/>
        <label class="btn display-status" for="pub9">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                DiCoDiLe: a distributed and asynchronous algorithm, employing locally greedy coordinate descent and an asynchronous locking mechanism that does not require a central server.
            </div>
            <div class="details">
                Convolutional dictionary learning (CDL) estimates shift invariant basis adapted to multidimensional data. CDL has proven useful for image denoising or inpainting, as well as for pattern discovery on multivariate signals. As estimated patterns can be positioned anywhere in signals or images, optimization techniques face the difficulty of working in extremely high dimensions with millions of pixels or time samples, contrarily to standard patch-based dictionary learning. To address this optimization problem, this work proposes a distributed and asynchronous algorithm, employing locally greedy coordinate descent and an asynchronous locking mechanism that does not require a central server. This algorithm can be used to distribute the computation on a number of workers which scales linearly with the encoded signal's size. Experiments confirm the scaling properties which allows us to learn patterns on large scales images from the Hubble Space Telescope.
            </div>
        </div>
    </div>

    
    <div class="publication">
        <div class="project_item">
            <span class="title">Sparsity-based blind deconvolution of neural activation signal in fMRI </span>
            <span style="width:2em;"></span>

            <a href="https://hal.inria.fr/hal-02085810v2/document">
                <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
            </a>


            <a href="https://github.com/CherkaouiHamza/pybold">
                <i class="fa fa-github-alt" aria-hidden="true"></i>
            </a>


            <br>
            <span class="authors">Hamza Cherkaoui, Thomas Moreau, Abderrahim Halimi, Philippe Ciuciu,</span>
            <span class="date">May 2019,</span>

                <span class="conference">In proceedings of <i>IEEE International Conference on Acoustic Speech and Signal Processing</i></span>


        </div>

        <input type="checkbox" class="btnCtrl" id="pub12"/>
        <label class="btn display-status" for="pub12">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                In this work, we formulate the joint estimation of the HRF and neural activation signal as a semi blind deconvolution problem.
            </div>
            <div class="details">
                The estimation of the hemodynamic response function (HRF) in functional magnetic resonance imaging (fMRI) is critical to deconvolve a time-resolved neural activity and get insights on the underlying cognitive processes. Existing methods pro-pose to estimate the HRF using the experimental paradigm(EP)  in  task  fMRI  as  a  surrogate  of  neural  activity.   These approaches  induce  a  bias  as  they  do  not  account  for  latencies in the cognitive responses compared to EP and cannot be applied to resting-state data as no EP is available.  In this work, we formulate the joint estimation of the HRF and neural activation signal as a semi blind deconvolution problem. Its solution can be approximated using an efficient alternate minimization algorithm.  The proposed approach is applied to task fMRI data for validation purpose and compared to a state-of-the-art HRF estimation technique.  Numerical experiments suggest that our approach is competitive with others while not requiring EP information.
            </div>
        </div>
    </div>

    
    <div class="publication">
        <div class="project_item">
            <span class="title">Multivariate Convolutional Sparse Coding for Electromagnetic Brain Signals </span>
            <span style="width:2em;"></span>

            <a href="https://arxiv.org/pdf/1805.09654.pdf">
                <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
            </a>


            <a href="https://github.com/tomMoral/alphacsc">
                <i class="fa fa-github-alt" aria-hidden="true"></i>
            </a>


            <br>
            <span class="authors">Tom Dupré La Tour, Thomas Moreau, Mainak Jas and Alexandre Gramfort,</span>
            <span class="date">Dec 2018,</span>

                <span class="conference">In proceedings of <i>Advances in Neural Information Processing System (NIPS)</i></span>


        </div>

        <input type="checkbox" class="btnCtrl" id="pub8"/>
        <label class="btn display-status" for="pub8">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                A multivariate CSC with rank-1 constrain algorithm designed to study brain activity waveforms
            </div>
            <div class="details">
                Frequency-specific patterns of neural activity are traditionally interpreted as sustained rhythmic oscillations, and related to cognitive mechanisms such as attention, high level visual processing or motor control. While alpha waves (8-12 Hz) are known to closely resemble short sinusoids, and thus are revealed by Fourier analysis or wavelet transforms, there is an evolving debate that electromagnetic neural signals are composed of more complex waveforms that cannot be analyzed by linear filters and traditional signal representations. In this paper, we propose to learn dedicated representations of such recordings using a multivariate convolutional sparse coding (CSC) algorithm. Applied to electroencephalography (EEG) or magnetoencephalography (MEG) data, this method is able to learn not only prototypical temporal waveforms, but also associated spatial patterns so their origin can be localized in the brain. Our algorithm is based on alternated minimization and a greedy coordinate descent solver that leads to state-of-the-art running time on long time series. To demonstrate the implications of this method, we apply it to MEG data and show that it is able to recover biological artifacts. More remarkably, our approach also reveals the presence of non-sinusoidal mu-shaped patterns, along with their topographic maps related to the somatosensory cortex.
            </div>
        </div>
    </div>

    
    <div class="publication">
        <div class="project_item">
            <span class="title">Template-Based Step Detection with Inertial Measurement Units </span>
            <span style="width:2em;"></span>

            <a href="Template-Based Step Detection with Inertial Measurement Units">
                <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
            </a>



            <br>
            <span class="authors">Laurent Oudre , Rémi Barrois-Müller, Thomas Moreau, Charles Truong, Aliénor Vienne-Jumeau, Damien Ricard, Nicolas Vayatis and Pierre-Paul Vidal,</span>
            <span class="date">Nov 2018,</span>

                <span><i>Sensors</i></span>


        </div>

        <input type="checkbox" class="btnCtrl" id="pub10"/>
        <label class="btn display-status" for="pub10">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                Step detection in inertial recordings using template based detection.
            </div>
            <div class="details">
                This article presents a method for step detection from accelerometer and gyrometer signals recorded with Inertial Measurement Units (IMUs). The principle of our step detection algorithm is to recognize the start and end times of the steps in the signal thanks to a predefined library of templates. The algorithm is tested on a database of 1020 recordings, composed of healthy subjects and patients with various neurological or orthopedic troubles. Simulations on more than 40,000 steps show that the template-based method achieves remarkable results with a 98% recall and a 98% precision. The method adapts well to pathological subjects and can be used in a medical context for robust step estimation and gait characterization.
            </div>
        </div>
    </div>

    
    <div class="publication">
        <div class="project_item">
            <span class="title">DICOD: Distributed Convolutional Coordinate Descent for Convolutional Sparse Coding </span>
            <span style="width:2em;"></span>

            <a href="http://proceedings.mlr.press/v80/moreau18a/moreau18a.pdf">
                <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
            </a>


            <a href="https://github.com/tomMoral/dicod">
                <i class="fa fa-github-alt" aria-hidden="true"></i>
            </a>


            <br>
            <span class="authors">Thomas Moreau, Laurent Oudre and Nicolas Vayatis,</span>
            <span class="date">Jul 2018,</span>

                <span class="conference">In proceedings of <i>International Conference on Machine Learning (ICML)</i></span>


        </div>

        <input type="checkbox" class="btnCtrl" id="pub6"/>
        <label class="btn display-status" for="pub6">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                In this paper, we introduce DICOD, a distributed convolutional sparse coding algorithm to build shift invariant representations for long signals.
            </div>
            <div class="details">
                In this paper, we introduce DICOD, a distributed convolutional sparse coding algorithm to build shift invariant representations for long signals. This algorithm is designed to run in a distributed setting, with local message passing, making it communication efficient. It is based on coordinate descent and uses locally greedy updates which accelerate the resolution compared to greedy coordinate selection. We prove the convergence of this algorithm and highlight its computational speed-up which is super-linear in the number of cores used. We also provide empirical evidence for the acceleration properties of our algorithm compared to state-of-the-art methods.
            </div>
        </div>
    </div>

    
    <div class="publication">
        <div class="project_item">
            <span class="title">Convolutional Sparse Representations -- application to physiological signals and interpretability for Deep Learning </span>
            <span style="width:2em;"></span>

            <a href="/publications/thesis.pdf">
                <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
            </a>



            <br>
            <span class="authors">Thomas Moreau,</span>
            <span class="date">Dec 2017,</span>

                <span><i>/</i></span>


        </div>

        <input type="checkbox" class="btnCtrl" id="pub11"/>
        <label class="btn display-status" for="pub11">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                Convolutional representations extract recurrent patterns which lead to the discovery of local structures in a set of signals. In this dissertation, we describe recent advances on both computational and theoretical aspects of these models.
            </div>
            <div class="details">
                Convolutional representations extract recurrent patterns which lead to the discovery of local structures in a set of signals. They are well suited to analyze physiological signals which requires interpretable representations in order to understand the relevant information. Moreover, these representations can be linked to deep learning models, as a way to bring interpretability in their internal representations. In this dissertation, we describe recent advances on both computational and theoretical aspects of these models. First, we show that the Singular Spectrum Analysis can be used to compute convolutional representations. This representation is dense and we describe an automatized procedure to improve its interpretability. Also, we propose an asynchronous algorithm, called DICOD, based on greedy coordinate descent, to solve convolutional sparse coding for long signals. Our algorithm has super-linear acceleration.In a second part, we focus on the link between representations and neural networks. An extra training step for deep learning, called post-training, is introduced to boost the performances of the trained network by making sure the last layer is optimal. Then, we study the mechanisms which allow to accelerate sparse coding algorithms with neural networks. We show that it is linked to a factorization of the Gram matrix of the dictionary.Finally, we illustrate the relevance of convolutional representations for physiological signals. Convolutional dictionary learning is used to summarize human walk signals and Singular Spectrum Analysis is used to remove the gaze movement in young infant’s oculometric recordings.
            </div>
        </div>
    </div>

    
    <div class="publication">
        <div class="project_item">
            <span class="title">Understanding the Learned Iterative Soft Thresholding Algorithm with matrix factorization </span>
            <span style="width:2em;"></span>

            <a href="https://arxiv.org/pdf/1706.01338.pdf">
                <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
            </a>



            <br>
            <span class="authors">Thomas Moreau and Joan Bruna,</span>
            <span class="date">Jun 2017,</span>

                <span><i>preprint Arxiv</i></span>


        </div>

        <input type="checkbox" class="btnCtrl" id="pub7"/>
        <label class="btn display-status" for="pub7">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                This paper aims to extend the results from our previous paper studying the mechanisms of LISTA and gives an acceleration certificate for generic dictionaries.
            </div>
            <div class="details">
                Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods are optimal in the class of first-order methods for non-smooth, convex functions. However, they do not exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks, coined LISTA, was proposed in Gregor and Le Cun (2010), which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately.</br>

In this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the ℓ1 ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.
            </div>
        </div>
    </div>

    
    <div class="publication">
        <div class="project_item">
            <span class="title">Understanding Trainable Sparse Coding with Matrix Factorization </span>
            <span style="width:2em;"></span>

            <a href="https://openreview.net/forum?id=SJGPL9Dex">
                <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
            </a>


            <a href="https://github.com/tomMoral/AdaptiveOptim">
                <i class="fa fa-github-alt" aria-hidden="true"></i>
            </a>


            <br>
            <span class="authors">Thomas Moreau and Joan Bruna,</span>
            <span class="date">Apr 2017,</span>

                <span class="conference">In proceedings of <i>International Conference on Learning Representations (ICLR)</i></span>


        </div>

        <input type="checkbox" class="btnCtrl" id="pub3"/>
        <label class="btn display-status" for="pub3">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                In this paper we study the mechanisms behind LISTA.
            </div>
            <div class="details">
                Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, that are optimal in the class of first-order methods for non-smooth, convex functions, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). However, these methods don't exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks was proposed in <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_GregorL10.pdf">Gregor10</a>, coined LISTA, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately.<br>
In this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the ℓ1 ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.
            </div>
        </div>
    </div>

    
    <div class="publication">
        <div class="project_item">
            <span class="title">Post Training in Deep Learning with Last Kernel </span>
            <span style="width:2em;"></span>

            <a href="https://arxiv.org/abs/1611.04499">
                <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
            </a>


            <a href="https://github.com/tomMoral/lastKernel">
                <i class="fa fa-github-alt" aria-hidden="true"></i>
            </a>


            <br>
            <span class="authors">Thomas Moreau and Julien Audiffren,</span>
            <span class="date">Nov 2016,</span>

                <span><i>preprint Arxiv</i></span>


        </div>

        <input type="checkbox" class="btnCtrl" id="pub2"/>
        <label class="btn display-status" for="pub2">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                Additional training step for deep networks to optimize the use of the features learnerd during the classical training. A link with existing kernel methods is then discussed.
            </div>
            <div class="details">
                One of the main challenges of deep learning methods is the choice of an appropriate training strategy. In particular, additional steps, such as unsupervised pre-training, have been shown to greatly improve the performances of deep structures. In this article, we propose an extra training step, called post-training, which only optimizes the last layer of the network. We show that this procedure can be analyzed in the context of kernel theory, with the first layers computing an embedding of the data and the last layer a statistical model to solve the task based on this embedding. This step makes sure that the embedding, or representation, of the data is used in the best possible way for the considered task. This idea is then tested on multiple architectures with various data sets, showing that it consistently provides a boost in performance.
            </div>
        </div>
    </div>

    
    <div class="publication">
        <div class="project_item">
            <span class="title">Distributed Convolutional Sparse Coding via Message Passing Interface </span>
            <span style="width:2em;"></span>

            <a href="http://www.cs.cmu.edu/~andrewgw/rep/MorOudVay.pdf">
                <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
            </a>



            <br>
            <span class="authors">Thomas Moreau, Laurent Oudre and Nicolas Vayatis,</span>
            <span class="date">Dec 2015,</span>

                <span class="conference">In <i>NIPS Workshop Nonparametric Methods for Large Scale Representation Learning</i></span>


        </div>

        <input type="checkbox" class="btnCtrl" id="pub1"/>
        <label class="btn display-status" for="pub1">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                Asynchronous algorithm to solve the convolutional sparse coding. This algorithm can be implemented using the MPI framework.
            </div>
            <div class="details">
                We consider the problem of building shift-invariant representations of signals from sensors with a large frequency of acquisition. We propose a distributed algorithm for convolutional sparse coding called DICOD that is based on coordinate descent and scales up with a speed up that is quadratic with respect to the number of processing units. Indeed, our implementation avoids sharing variables between cores and does not require any lock or synchronization at every step. We present theoretical results and empirical evidence of convergence of DICOD, and also provide numerical comparisons with respect to widely used algorithms for convolutional sparse coding.
            </div>
        </div>
    </div>

    
    <div class="publication">
        <div class="project_item">
            <span class="title">Groupement automatique pour l’analyse du spectre singulier </span>
            <span style="width:2em;"></span>

            <a href="http://www.laurentoudre.fr/publis/MOV-GRETSI-15.pdf">
                <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
            </a>



            <br>
            <span class="authors">Thomas Moreau, Laurent Oudre and Nicolas Vayatis,</span>
            <span class="date">Sep 2015,</span>

                <span class="conference">In proceedings of <i>the Groupe de Recherche et d&#39;Etudes en Traitement du Signal et des Images (GRETSI)</i></span>


        </div>

        <input type="checkbox" class="btnCtrl" id="pub4"/>
        <label class="btn display-status" for="pub4">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                This paper introduces several automatic grouping strategies for Singular Spectrum Analysis (SSA) components in a unified framework.
            </div>
            <div class="details">
                This paper introduces several automatic grouping strategies for Singular Spectrum Analysis (SSA) components. This step is useful to retrieve meaningful insight about the temporal dynamics of the series. A unifying framework is proposed to evaluate and compare the efficiency of different original methods compared to the existing ones.
            </div>
        </div>
    </div>

    
    <div class="publication">
        <div class="project_item">
            <span class="title">Détection de pas à partir de données d&#39;accélérométrie </span>
            <span style="width:2em;"></span>



            <br>
            <span class="authors">Laurent Oudre , Thomas Moreau , Charles Truong , Rémi Barrois-Müller , Robert Dadashi  and Thomas Grégory,</span>
            <span class="date">Sep 2015,</span>

                <span class="conference">In proceedings of <i>the Groupe de Recherche et d&#39;Etudes en Traitement du Signal et des Images (GRETSI)</i></span>


        </div>

        <input type="checkbox" class="btnCtrl" id="pub5"/>
        <label class="btn display-status" for="pub5">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                This article presents a method for step detection from accelerometer signals based on template matching.
            </div>
            <div class="details">
                This article presents a method for step detection from accelerometer signals based on template matching. This method uses a library of step templates extracted from real data in order to not only count the steps but also to retrieve the start and end times of each step. The algorithm is tested on a large database of 300 recordings, composed of healthy patients and patients with various orthopaedic troubles. Simulations show that even with only 20 templates, our method achieves remarkable results with a 97% recall and a 96% precision, is robust and adapts well to pathological subjects.
            </div>
        </div>
    </div>


	</div>
</article>
    </div>
</div><!-- #page -->

<div style="display:none">
</div>

</body>

<script type="text/javascript">
    window.onresize = function(){
        var sidebar = document.getElementById("sidebar");
        var page = document.getElementById("page");
        page.style.setProperty("min-height", sidebar.clientHeight);
    }
    window.onresize()
</script>
</html>
