
<html lang="en-US">
<!--<![endif]-->
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width" />
<title>Thomas Moreau</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<!--[if lt IE 9]>
<script src="http://dpkingma.com/wordpress/wp-content/themes/twentytwelve/js/html5.js" type="text/javascript"></script>
<![endif]-->
<script src="https://use.fontawesome.com/ae904b9937.js"></script>



<link rel="shortcut icon" href="/images/favicon.png">
<link rel="stylesheet" type="text/css" href="/css/style.css" />
<link rel="stylesheet" href="/css/academicons.css"/>
<script type="text/javascript" src="/javascript/lib.js"></script>
<!-- highlight current page in navigation -->
<script>
$(function(){
  $('a').each(function() {
    if ($(this).prop('href') == window.location.href) {
      $(this).parent().addClass('current-menu-item');
    }
  });
});
</script>

</head>

<body class="home">
<div id="sidebar">
    <header id="masthead" class="site-header" role="banner">
        <hgroup>
            <img class="profile_img" src="https://s.gravatar.com/avatar/1c7c7939d1173cd6f455ce4313e831c5?s=150&r=x" />
            <div class="info">
                <h2 class="site-title"><a href="/about">Thomas Moreau</a></h2>
                <h4 class="site-description">Parietal - Inria Saclay</h4>
                <h5 class="email"> thomas.moreau [AT] inria.fr</h5>
            </div>
        </hgroup>

        <nav id="site-navigation" class="main-navigation" role="navigation">
            <div class="menu-menu-1-container"><ul id="menu-nav" class="menu-nav">
                <li class="nav-menu-item"><a href="/about">About</a></li>
                <li class="nav-menu-item"><a href="/publications">Publications</a></li>
                <li class="nav-menu-item"><a href="/talks">Talks and Videos</a></li>
                <li class="nav-menu-item"><a href="/oss">Open Source Software</a></li>
            </ul></div>
        </nav><!-- #site-navigation -->
        <div id="social-media">
            <a href="https://github.com/tomMoral">
                <i class="fa fa-github fa-3x"></i></a>
            <a href="https://scholar.google.fr/citations?user=HEO_PsAAAAAJ">
                <i class="ai ai-google-scholar ai-3x"></i></a>
            <a href="https://stackoverflow.com/story/tomMoral">
                <i class="fa fa-stack-overflow fa-3x"></i></a>
            <a href="https://linkedin.com/in/tomMoral">
                <i class="fa fa-linkedin fa-3x"></i></a>
        </div>
    </header><!-- #masthead -->
</div>
</div>

<div id="page">
    <div class="page-content">
    <article class="inner-text">
    <header class="entry-header">
        <h1 class="entry-title">Presentations</h1>
    </header>

    <div class="entry-content">


        <div class="talk">
        <div class="talk_item">
            <span class="title">Multivariate Convolutional Sparse Coding for Electromagnetic Brain Signals </span>
            <span style="width:2em;"></span>

            <a href="/talks/20_02_25_PASADENA_multicsc.pdf">

                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>


            <br>
            <span class="date">26 Feb 2020,</span>
            <span class="conference">At <i>GT PASADENA</i></span>

        </div>

        <input type="checkbox" class="btnCtrl" id="talk16"/>
        <label class="btn display-status" for="talk16">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                Convolutional dictionary learning algorithm adapted to capture waveforms from MEG signals.
            </div>
            <div class="details">
                Frequency-specific patterns of neural activity are traditionally interpreted as sustained rhythmic oscillations, and related to cognitive mechanisms such as attention, high level visual processing or motor control. While alpha waves (8--12\, Hz) are known to closely resemble short sinusoids, and thus are revealed by Fourier analysis or wavelet transforms, there is an evolving debate that electromagnetic neural signals are composed of more complex waveforms that cannot be analyzed by linear filters and traditional signal representations. In this paper, we propose to learn dedicated representations of such recordings using a multivariate convolutional sparse coding (CSC) algorithm. Applied to electroencephalography (EEG) or magnetoencephalography (MEG) data, this method is able to learn not only prototypical temporal waveforms, but also associated spatial patterns so their origin can be localized in the brain. Our algorithm is based on alternated minimization and a greedy coordinate descent solver that leads to state-of-the-art running time on long time series. To demonstrate the implications of this method, we apply it to MEG data and show that it is able to recover biological artifacts. More remarkably, our approach also reveals the presence of non-sinusoidal mu-shaped patterns, along with their topographic maps related to the somatosensory cortex.
            </div>
        </div>
    </div>


        <div class="talk">
        <div class="talk_item">
            <span class="title">Learning step sizes for unfolded sparse coding </span>
            <span style="width:2em;"></span>

            <a href="/talks/20_01_13_SemOpt_slista.pdf">

                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>


            <br>
            <span class="date">13 Jan 2020,</span>
            <span class="conference">At <i>Seminaire Optimization - IHP, Paris</i></span>

        </div>

        <input type="checkbox" class="btnCtrl" id="talk15"/>
        <label class="btn display-status" for="talk15">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                Unfolding and learning weights of ISTA using neural networks is a practical way to accelerate the Lasso resolution. However, the reason why learning the weights of such a network would accelerate sparse coding are not clear. In this talk, we look at this problem from the point of view of selecting adapted step sizes for ISTA.
            </div>
            <div class="details">
                Sparse coding is typically solved by iterative optimization techniques, such as the Iterative Shrinkage-Thresholding Algorithm (ISTA). Unfolding and learning weights of ISTA using neural networks is a practical way to accelerate estimation. However, the reason why learning the weights of such a network would accelerate sparse coding are not clear. In this talk, we look at this problem from the point of view of selecting adapted step sizes for ISTA. We show that a simple step size strategy can improve the convergence rate of ISTA by leveraging the sparsity of the iterates. However, it is impractical in most large-scale applications. Therefore, we propose a network architecture where only the step sizes of ISTA are learned. We demonstrate if the learned algorithm converges to the solution of the Lasso, its last layers correspond to ISTA with learned step sizes. Experiments show that learning step sizes can effectively accelerate the convergence when the solutions are sparse enough.
            </div>
        </div>
    </div>


        <div class="talk">
        <div class="talk_item">
            <span class="title">Learning step sizes for unfolded sparse coding </span>
            <span style="width:2em;"></span>

            <a href="/talks/19_07_02_slista.pdf">

                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>


            <br>
            <span class="date">02 Jul 2019,</span>
            <span class="conference">At <i>Parietal seminar - INRIA, Saclay</i></span>

        </div>

        <input type="checkbox" class="btnCtrl" id="talk14"/>
        <label class="btn display-status" for="talk14">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                In this talk, we show that adaptive analytic step sizes can be used to improved ISTA. This steps can also be learned using a neural network architecture simlar to LISTA, which is theoretically the only way to learn to accelerate  ISTA asymptotically.
            </div>
            <div class="details">
                Sparse coding is typically solved by iterative optimization techniques, such as the Iterative Shrinkage-Thresholding Algorithm (ISTA). Unfolding and learning weights of ISTA using neural networks is a practical way to accelerate estimation. In this paper, we study the selection of adapted step sizes for ISTA. We show that a simple step size strategy can improve the convergence rate of ISTA by leveraging the sparsity of the iterates. However, it is impractical in most large-scale applications. Therefore, we propose a network architecture where only the step sizes of ISTA are learned. We demonstrate that for a large class of unfolded algorithms, if the algorithm converges to the solution of the Lasso, its last layers correspond to ISTA with learned step sizes. Experiments show that our method is competitive with state-of-the-art networks when the solutions are sparse enough.
            </div>
        </div>
    </div>


        <div class="talk">
        <div class="talk_item">
            <span class="title">Best Practices & Pitfalls in Applying Machine Learning to Magnetic Resonance Imaging </span>
            <span style="width:2em;"></span>

            <a href="/talks/19_05_11_ISMRM_ML_for_MRI.pdf">

                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>


            <br>
            <span class="date">11 May 2019,</span>
            <span class="conference">At <i>Invited talk @ ISMRM - Montreal, Canada</i></span>

        </div>

        <input type="checkbox" class="btnCtrl" id="talk13"/>
        <label class="btn display-status" for="talk13">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                In this talk, I cover the concept of generalization for supervised learning, with a focus on model selection and the importance of sample size.
            </div>
            <div class="details">
                In this talk, I cover the concept of generalization for supervised learning, with a focus on model selection and the importance of sample size.
            </div>
        </div>
    </div>


        <div class="talk">
        <div class="talk_item">
            <span class="title">Distributed Convolutional Dictionary Learning (DiCoDiLe): Pattern Discovery in Large Images and Signals </span>
            <span style="width:2em;"></span>

            <a href="/talks/19_04_16_PARIETAL_dicodile.pdf">

                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>


            <br>
            <span class="date">16 Apr 2019,</span>
            <span class="conference">At <i>Parietal seminar - INRIA, Saclay</i></span>

        </div>

        <input type="checkbox" class="btnCtrl" id="talk12"/>
        <label class="btn display-status" for="talk12">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                DiCoDiLe: a distributed and asynchronous algorithm, employing locally greedy coordinate descent and an asynchronous locking mechanism that does not require a central server.
            </div>
            <div class="details">
                Convolutional dictionary learning (CDL) estimates shift invariant basis adapted to multidimensional data. CDL has proven useful for image denoising or inpainting, as well as for pattern discovery on multivariate signals. As estimated patterns can be positioned anywhere in signals or images, optimization techniques face the difficulty of working in extremely high dimensions with millions of pixels or time samples, contrarily to standard patch-based dictionary learning. To address this optimization problem, this work proposes a distributed and asynchronous algorithm, employing locally greedy coordinate descent and an asynchronous locking mechanism that does not require a central server. This algorithm can be used to distribute the computation on a number of workers which scales linearly with the encoded signal's size. Experiments confirm the scaling properties which allows us to learn patterns on large scales images from the Hubble Space Telescope.
            </div>
        </div>
    </div>


        <div class="talk">
        <div class="talk_item">
            <span class="title">Multivariate Convolutional Sparse Coding for Electromagnetic Brain Signals </span>
            <span style="width:2em;"></span>

            <a href="/talks/19_03_28_SMILE_multicsc.pdf">

                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>


            <br>
            <span class="date">28 Mar 2019,</span>
            <span class="conference">At <i>Séminaire SMILE - Paris</i></span>

        </div>

        <input type="checkbox" class="btnCtrl" id="talk10"/>
        <label class="btn display-status" for="talk10">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                Convolutional dictionary learning algorithm adapted to capture waveforms from MEG signals.
            </div>
            <div class="details">
                Frequency-specific patterns of neural activity are traditionally interpreted as sustained rhythmic oscillations, and related to cognitive mechanisms such as attention, high level visual processing or motor control. While alpha waves (8--12\, Hz) are known to closely resemble short sinusoids, and thus are revealed by Fourier analysis or wavelet transforms, there is an evolving debate that electromagnetic neural signals are composed of more complex waveforms that cannot be analyzed by linear filters and traditional signal representations. In this paper, we propose to learn dedicated representations of such recordings using a multivariate convolutional sparse coding (CSC) algorithm. Applied to electroencephalography (EEG) or magnetoencephalography (MEG) data, this method is able to learn not only prototypical temporal waveforms, but also associated spatial patterns so their origin can be localized in the brain. Our algorithm is based on alternated minimization and a greedy coordinate descent solver that leads to state-of-the-art running time on long time series. To demonstrate the implications of this method, we apply it to MEG data and show that it is able to recover biological artifacts. More remarkably, our approach also reveals the presence of non-sinusoidal mu-shaped patterns, along with their topographic maps related to the somatosensory cortex.
            </div>
        </div>
    </div>


        <div class="talk">
        <div class="talk_item">
            <span class="title">Multivariate Convolutional Sparse Coding for Electromagnetic Brain Signals </span>
            <span style="width:2em;"></span>

            <a href="/talks/19_02_19_MLMDA_multicsc_dicodile.pdf">

                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>


            <br>
            <span class="date">19 Feb 2019,</span>
            <span class="conference">At <i>MLMDA seminar - CMLA, ENS Paris-Saclay</i></span>

        </div>

        <input type="checkbox" class="btnCtrl" id="talk11"/>
        <label class="btn display-status" for="talk11">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                Rank-1 constrined Convolutional Dictionary Learning algorithm for neural time series and distributed algorithm for CDL (DiCoDiLe)
            </div>
            <div class="details">
                Frequency-specific patterns of neural activity are traditionally interpreted as sustained rhythmic oscillations, and related to cognitive mechanisms such as attention, high level visual processing or motor control. While alpha waves (8--12\,Hz) are known to closely resemble short sinusoids, and thus are revealed by Fourier analysis or wavelet transforms, there is an evolving debate that electromagnetic neural signals are composed of more complex waveforms that cannot be analyzed by linear filters and traditional signal representations. In this paper, we propose to learn dedicated representations of such recordings using a multivariate convolutional sparse coding (CSC) algorithm. Applied to electroencephalography (EEG) or magnetoencephalography (MEG) data, this method is able to learn not only prototypical temporal waveforms, but also associated spatial patterns so their origin can be localized in the brain. Our algorithm is based on alternated minimization and a greedy coordinate descent solver that leads to state-of-the-art running time on long time series. To demonstrate the implications of this method, we apply it to MEG data and show that it is able to recover biological artifacts. More remarkably, our approach also reveals the presence of non-sinusoidal mu-shaped patterns, along with their topographic maps related to the somatosensory cortex. </br>
In a second part, I will present my recent work on a distributed algorithm for CDL, called DiCoDiLe.
            </div>
        </div>
    </div>


        <div class="talk">
        <div class="talk_item">
            <span class="title">Using the Dictionary Structure for efficient Convolutional Dictionary Learning </span>
            <span style="width:2em;"></span>

            <a href="/talks/19_01_31_telecom.pdf">

                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>


            <br>
            <span class="date">31 Jan 2019,</span>
            <span class="conference">At <i>Seminaire du LTCI - Télécom paritech</i></span>

        </div>

        <input type="checkbox" class="btnCtrl" id="talk9"/>
        <label class="btn display-status" for="talk9">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                Convolutional Dictionary Learning (CDL) is a popular tool in image processing for denoising or inpainting. This technique extends dictionary learning to learn adapted basis that are shift invariant. It can also be used in the context of large multivariate signals to learn and localize recurring patterns. In this talk, I will focus on how the structure of the dictionary can be leveraged to design efficient CDL algorithm and to improve the interpretability of the recovered patterns in MEG signals.
            </div>
            <div class="details">
                Convolutional Dictionary Learning (CDL) is a popular tool in image processing for denoising or inpainting. This technique extends dictionary learning to learn adapted basis that are shift invariant. It can also be used in the context of large multivariate signals to learn and localize recurring patterns. In this talk, I will focus on how the structure of the dictionary can be leveraged to design efficient CDL algorithm and to improve the interpretability of the recovered patterns in MEG signals.</br>
</br>
Gregor and Le Cun (2010) have shown empirically it is possible to estimate the solution of the sparse coding with a neural network called LISTA trained with back-propagation. In the first part of this talk, I will show the link between the performance of this network and the particular structure of the dictionary and in particular the Gram matrix of the problem. This structure is shown to be sufficient to explain the performance of LISTA and numerical experiments show it is also necessary. (Joint work with J. Bruna).</br>
</br>
Then, I will focus on the convolutional sparse coding (CSC). The particular properties of the band-circulant matrices allow to derive an efficient algorithm, called Locally Greedy Coordinate Descent (LGCD), to solve the CSC. This algorithm is very efficient in this context as it prioritizes important updates, still managing low iteration complexity. This algorithm can also be used in an asynchronous and distributed setting. We show that even though the updates are only performed locally, without global communication, this distributed algorithm converges to the optimal solution of the CSC. Numerical experiments show that this algorithm has better performance than state-of-the-art algorithms for CSC and that it accelerates super linearly with the number of cores used. (Joint work with L. Oudre, N. Vayatis and A. Gramfort).</br>
</br>
In the final part of my talk, I will introduce a novel rank 1 constraint for the learned atoms for practical problems in neuroscience. This constraint, inspired from the underlying physical model for neurological signals, is then used to highlight relevant structure in MEG signals. (Join work with T. Dupré la Tour, M. Jas and A. Gramfort).
            </div>
        </div>
    </div>


        <div class="talk">
        <div class="talk_item">
            <span class="title">Learning Recurring Patterns in Large Signals with Convolutional Dictionary Learning </span>
            <span style="width:2em;"></span>

            <a href="/talks/19_01_24_cosmostat.pdf">

                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>


            <br>
            <span class="date">24 Jan 2019,</span>
            <span class="conference">At <i>Seminar day - CosmoStat, CEA, Orsay</i></span>

        </div>

        <input type="checkbox" class="btnCtrl" id="talk8"/>
        <label class="btn display-status" for="talk8">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                How convolutional dictionary learning can be used in the context of large multivariate signals to learn and localize recurring patterns: modelisation and computational aspects.
            </div>
            <div class="details">
                Convolutional dictionary learning has become a popular tool in image processing for denoising or inpainting. This technique extends dictionary learning to learn adapted basis that are shift invariant. This talk will discuss how this technique can also be used in the context of large multivariate signals to learn and localize recurring patterns. I will discuss both computational aspects, with efficient iterative and distributed convolutional sparse coding algorithms, as well as a novel rank 1 constraint for the learned atoms. This constraint, inspired from the underlying physical model for neurological signals, is then used to highlight relevant structure in MEG signals.
            </div>
        </div>
    </div>


        <div class="talk">
        <div class="talk_item">
            <span class="title">DICOD: Distributed Convolutional Coordinate Descent for Convolutional Sparse Coding </span>
            <span style="width:2em;"></span>

            <a href="/talks/18_07_12_dicod.pdf">

                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>


            <a href="https://www.facebook.com/icml.imls/videos/session-1-parallel-and-distributed-learning/430850664096121/">
                <i class="fa fa-video-camera" aria-hidden="true"></i>
            </a>

            <br>
            <span class="date">12 Jul 2018,</span>
            <span class="conference">At <i>ICML - Stockholm, Sweden</i></span>

        </div>

        <input type="checkbox" class="btnCtrl" id="talk7"/>
        <label class="btn display-status" for="talk7">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                In this talk, we introduce DICOD, a distributed  convolutional sparse coding algorithm to build shift invariant representations for long signals
            </div>
            <div class="details">
                In this talk, we introduce DICOD, a  distributed convolutional sparse coding algorithm to build shift invariant representations for long signals. This algorithm is designed to run in a distributed setting, with local message passing, making it communication efficient. It is based on coordinate descent and uses locally greedy updates which accelerate the resolution compared to greedy coordinate selection. We prove the convergence of this algorithm and highlight its computational speed-up which is super-linear in the number of cores used. We also provide empirical evidence for the acceleration properties of our algorithm compared to state-of-the-art methods.
            </div>
        </div>
    </div>


        <div class="talk">
        <div class="talk_item">
            <span class="title">Multivariate Convolutional Sparse Coding for Electromagnetic Brain Signals </span>
            <span style="width:2em;"></span>

            <a href="/talks/18_05_29_multicsc.pdf">

                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>


            <br>
            <span class="date">29 May 2018,</span>
            <span class="conference">At <i>Parietal seminar - INRIA, Saclay</i></span>

        </div>

        <input type="checkbox" class="btnCtrl" id="talk4"/>
        <label class="btn display-status" for="talk4">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                A multivariate CSC with rank-1 constrain algorithm designed to study brain activity waveforms
            </div>
            <div class="details">
                Frequency-specific patterns of neural activity are traditionally interpreted as sustained rhythmic oscillations, and related to cognitive mechanisms such as attention, high level visual processing or motor control. While alpha waves (8-12 Hz) are known to closely resemble short sinusoids, and thus are revealed by Fourier analysis or wavelet transforms, there is an evolving debate that electromagnetic neural signals are composed of more complex waveforms that cannot be analyzed by linear filters and traditional signal representations. In this paper, we propose to learn dedicated representations of such recordings using a multivariate convolutional sparse coding (CSC) algorithm. Applied to electroencephalography (EEG) or magnetoencephalography (MEG) data, this method is able to learn not only prototypical temporal waveforms, but also associated spatial patterns so their origin can be localized in the brain. Our algorithm is based on alternated minimization and a greedy coordinate descent solver that leads to state-of-the-art running time on long time series. To demonstrate the implications of this method, we apply it to MEG data and show that it is able to recover biological artifacts. More remarkably, our approach also reveals the presence of non-sinusoidal mu-shaped patterns, along with their topographic maps related to the somatosensory cortex.
            </div>
        </div>
    </div>


        <div class="talk">
        <div class="talk_item">
            <span class="title">PhD defense </span>
            <span style="width:2em;"></span>

            <a href="/talks/17_12_19_defense.pdf">

                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>


            <br>
            <span class="date">19 Dec 2017,</span>
            <span class="conference">At <i>CMLA - ENS Paris-Saclay</i></span>

        </div>

        <input type="checkbox" class="btnCtrl" id="talk5"/>
        <label class="btn display-status" for="talk5">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                Convolutional Sparse Representations -- application to physiological signals and interpretability for Deep Learning
            </div>
            <div class="details">
                Convolutional representations extract recurrent patterns which lead to the discovery of local structures in a set of signals. They are well suited to analyze physiological signals which requires interpretable representations in order to understand the relevant information. Moreover, these representations can be linked to deep learning models, as a way to bring interpretability in their internal representations. In this dissertation, we describe recent advances on both computational and theoretical aspects of these models.<br/></br>
Our main contribution in the first part is an asynchronous algorithm, called DICOD, based on greedy coordinate descent, to solve convolutional sparse coding for long signals. Our algorithm has super-linear acceleration. We also explored the relationship of Singular Spectrum Analysis with convolutional representations, as an initialization step for convolutional dictionary learning.<br/></br>
In a second part, we focus on the link between representations and neural networks. Our main result is a study of the mechanisms which accelerate sparse coding algorithms with neural networks. We show that it is linked to a factorization of the Gram matrix of the dictionary. Other aspects of representations in neural networks are also investigated with an extra training step for deep learning, called post-training, to boost the performances of trained networks by improving their last layer's weights.<br/></br>
Finally, we illustrate the relevance of convolutional representations for physiological signals. Convolutional dictionary learning is used to summarize signals from human walking and Singular Spectrum Analysis is used to remove the gaze movement in young infant's oculometric recordings.
            </div>
        </div>
    </div>


        <div class="talk">
        <div class="talk_item">
            <span class="title">Accelerating sparse coding resolution </span>
            <span style="width:2em;"></span>

            <a href="/talks/17_12_01_MAP5_seminar.pdf">

                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>


            <br>
            <span class="date">01 Dec 2017,</span>
            <span class="conference">At <i>séminaire de statistique - MAP5, Paris</i></span>

        </div>

        <input type="checkbox" class="btnCtrl" id="talk6"/>
        <label class="btn display-status" for="talk6">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                Acceleration strategies for sparse coding resolution, using the optmization structure.
            </div>
            <div class="details">
                Sparse coding is a core building block in many data analysis and machine learning pipelines. Finding good algorithms to accelerate the resolution of such problem is thus critical to many  applications.<br/></br>
The first part of this talk is focused on recent acceleration techniques which estimate the sparse code with a train neural network such as LISTA. Empirical results have shown that they achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately. In this talk, I will link the performance of these network to a factorization of the Gram matrix of the problem which preserves the l1 norm. This mechanism is shown to be sufficient to explain the performance of LISTA and numerical experiments show that it is also necessary. (Joint work with J. Bruna)</br></br>
In a second part of the talk, I will focus on convolutional sparse coding, with band circulant matrices. The particular properties of these problems allow to derive an efficient distributed algorithm based on the greedy coordinate descent. It can be shown that this algorithm converges in an asynchronous setting, is communication efficient and has a super-linear speed-up. These different properties are then illustrated with numerical experiments. (Joint work with N. Vayatis and L. Oudre)
            </div>
        </div>
    </div>


        <div class="talk">
        <div class="talk_item">
            <span class="title">Understanding Trainable Sparse Coding with Matrix Factorization </span>
            <span style="width:2em;"></span>

            <a href="/talks/17_11_Google_ZRH.pdf">

                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>


            <br>
            <span class="date">06 Nov 2017,</span>
            <span class="conference">At <i>Tech talk - Google, Zurich</i></span>

        </div>

        <input type="checkbox" class="btnCtrl" id="talk3"/>
        <label class="btn display-status" for="talk3">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                In this talk, we provide elements explaining why Learned ISTA is able to accelerate the LASSO resolution. This vision of optimization algorithms in the neural network framework could be used to link sparse representations and neural networks.
            </div>
            <div class="details">
                Optimization algorithms for sparse coding can be viewed in the light of the neural network framework. Using this framework, it is possible to design trainable networks which accelerate the resolution of an optimization problem on a given distribution, as it has been shown with the Learned ISTA network, proposed by Gregor & Le Cun (2010).</br></br>
In this talk, we provide elements explaining why the acceleration is possible in the case of ISTA. We show that the resolution of sparse coding can be accelerated compared to ISTA when the design matrix admits a quasi-diagonal factorization with sparse eigenspaces. The resulting algorithm has the same convergence rate but an improved constant factor. Then we show under which conditions such factorization is possible with high probability for generic Gaussian dictionaries. Finally, we design neural networks which compute this algorithm and show that they are a re-parametrization of LISTA. Thus, the performance of LISTA are at least as good as this algorithm. We conclude by designing adverse examples for our factorization based algorithm and show that LISTA also fails to accelerate on these cases, proving that this mechanism plays a role in LISTA acceleration.
            </div>
        </div>
    </div>


        <div class="talk">
        <div class="talk_item">
            <span class="title">Understanding physiological signals via sparse representations </span>
            <span style="width:2em;"></span>

            <a href="/talks/17_10_EDMH.pdf">

                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>


            <br>
            <span class="date">09 Oct 2017,</span>
            <span class="conference">At <i>Journée de rentré EDMH - IHES, Orsay</i></span>

        </div>

        <input type="checkbox" class="btnCtrl" id="talk2"/>
        <label class="btn display-status" for="talk2">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                General presentation of time series representations and of the convolutional representation model.
            </div>
            <div class="details">
                General presentation of time series representations and of the convolutional representation model.
            </div>
        </div>
    </div>


        <div class="talk">
        <div class="talk_item">
            <span class="title">Robustifying <span class="highlightcode">concurrent.futures</span> with <span class="highlightcode">loky</span> </span>
            <span style="width:2em;"></span>

            <a href="/talks/pyparis17">

                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>


            <a href="https://youtu.be/VpLBrWPOYQ8">
                <i class="fa fa-video-camera" aria-hidden="true"></i>
            </a>

            <br>
            <span class="date">12 Jun 2017,</span>
            <span class="conference">At <i>PyParis 2017 - Paris</i></span>

        </div>

        <input type="checkbox" class="btnCtrl" id="talk1"/>
        <label class="btn display-status" for="talk1">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                The <span class="highlightcode">concurrent.futures</span> module offers an easy to use API to parallelize code execution in python, using <span class="highlightcode">threading</span> or <span class="highlightcode">multiprocessing</span> primitives. We will begin our talk by presenting this API and the differences between <span class="highlightcode">Thread</span> and <span class="highlightcode">Process</span>.
            </div>
            <div class="details">
                The <span class="highlightcode">concurrent.futures</span> module offers an easy to use API to parallelize code execution in python, using <span class="highlightcode">threading</span> or <span class="highlightcode">multiprocessing</span> primitives. We will begin our talk by presenting this API and the differences between <span class="highlightcode">Thread</span> and <span class="highlightcode">Process</span>.</br>
</br>
For <span class="highlightcode">Process</span> backed executions, useful for pure python parallelization, several issues can reduce the performance. Spawning new workers for each execution can create a large overhead, but maintaining the pool of worker across the program can become quickly bothersome. We will describe several of the pitfall that can make using <span class="highlightcode">concurrent.futures</span> unstable.</br>
</br>
Finally, we will introduce <span class="highlightcode">loky</span>, a library providing a robust, reusable pool of workers, handled internally. It uses a customized implementation of <span class="highlightcode">ProcessPoolExecutor</span> from <span class="highlightcode">concurrent.futures</span>. We will describe its main features and the major technical design choice that helped making it more robust.
            </div>
        </div>
    </div>



    </div>
</article>
    </div>
</div><!-- #page -->

<div style="display:none">
</div>

</body>

<script type="text/javascript">
    window.onresize = function(){
        var sidebar = document.getElementById("sidebar");
        var page = document.getElementById("page");
        page.style.setProperty("min-height", sidebar.clientHeight);
    }
    window.onresize()
</script>
</html>
