
<html lang="en-US">
<!--<![endif]-->
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width" />
<title>Thomas Moreau</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<!--[if lt IE 9]>
<script src="http://dpkingma.com/wordpress/wp-content/themes/twentytwelve/js/html5.js" type="text/javascript"></script>
<![endif]-->
<script src="https://use.fontawesome.com/ae904b9937.js"></script>



<link rel="shortcut icon" href="/images/favicon.png">
<link rel="stylesheet" type="text/css" href="/css/style.css" />
<link rel="stylesheet" href="/css/academicons.css"/>
<script type="text/javascript" src="/javascript/lib.js"></script>
<!-- highlight current page in navigation -->
<script>
$(function(){
  $('a').each(function() {
    if ($(this).prop('href') == window.location.href) {
      $(this).parent().addClass('current-menu-item');
    }
  });
});
</script>

</head>

<body class="home">
<div id="sidebar">
    <header id="masthead" class="site-header" role="banner">
        <hgroup>
            <img class="profile_img" src="https://s.gravatar.com/avatar/1c7c7939d1173cd6f455ce4313e831c5?s=150&r=x" />
            <div class="info">
                <h2 class="site-title"><a href="/about">Thomas Moreau</a></h2>
                <h4 class="site-description">Parietal - Inria Saclay</h4>
                <h5 class="email"> thomas.moreau [AT] inria.fr</h5>
            </div>
        </hgroup>

        <nav id="site-navigation" class="main-navigation" role="navigation">
            <div class="menu-menu-1-container"><ul id="menu-nav" class="menu-nav">
                <li class="nav-menu-item"><a href="/about">About</a></li>
                <li class="nav-menu-item"><a href="/publications">Publications</a></li>
                <li class="nav-menu-item"><a href="/talks">Talks and Videos</a></li>
                <li class="nav-menu-item"><a href="/oss">Open Source Software</a></li>
            </ul></div>
        </nav><!-- #site-navigation -->
        <div id="social-media">
            <a href="https://github.com/tomMoral">
                <i class="fa fa-github fa-3x"></i></a>
            <a href="https://scholar.google.fr/citations?user=HEO_PsAAAAAJ">
                <i class="ai ai-google-scholar ai-3x"></i></a>
            <a href="https://stackoverflow.com/story/tomMoral">
                <i class="fa fa-stack-overflow fa-3x"></i></a>
            <a href="https://linkedin.com/in/tomMoral">
                <i class="fa fa-linkedin fa-3x"></i></a>
        </div>
    </header><!-- #masthead -->
</div>
</div>

<div id="page">
    <div class="page-content">
    <article class="inner-text">
    <header class="entry-header">
        <h1 class="entry-title">About me</h1>
    </header>

    <div class="entry-content">
	<p>
        I am a researcher in the parietal team @ INRIA Saclay, since autumn 2019, working on unsupervised learning for time series and on deep learning methods applied to solving inverse problems.
	</p>

	<p>
        My research interests touch several areas of Machine Learning, Signal Processing and High-Dimensional Statistics. In particular, I have been working on Convolutional Dictionary Learning, studying both their computational aspects and their possible application to pattern analysis. I am also interested in theoretical properties of learned optimization algorithms such as LISTA, in particular for the resolution of inverse problems.
        </p>

	<p>
        I did my PhD under the supervision of Nicolas Vayatis and Laurent Oudre, in the CMLA @ ENS Paris-Saclay. My PhD focuses on convolutional representations and their applications to physiological signals. I continued working on unsupervised learning for time series with application to electrophysiological recordings during a 1,5 year in the Parietal Team. I am also involved in open-source projects such as <span class="highlightcode">joblib</span> or <span class="highlightcode">loky</span>, for parallel scientific computing.
	</p>

	<h3>Latest publication and projects</h3>
        
    <div class="publication">
        <div class="project_item">
            <span class="title">Super-efficiency of automatic differentiation for functions defined as a minimum </span>
            <span style="width:2em;"></span>

            <a href="https://arxiv.org/pdf/2002.03722">
                <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
            </a>



            <br>
            <span class="authors">Pierre Ablin; Gabriel Peyr√© and Thomas Moreau,</span>
            <span class="date">Feb 2020,</span>

                <span><i>preprint /</i></span>


        </div>

        <input type="checkbox" class="btnCtrl" id="pub15"/>
        <label class="btn display-status" for="pub15">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                We study the different techniques to differentiate a function defined as a min of an other.
            </div>
            <div class="details">
                In min-min optimization or max-min optimization, one has to compute the gradient of a function defined as a minimum. In most cases, the minimum has no closed-form, and an approximation is obtained via an iterative algorithm. There are two usual ways of estimating the gradient of the function: using either an analytic formula obtained by assuming exactness of the approximation, or automatic differentiation through the algorithm. In this paper, we study the asymptotic error made by these estimators as a function of the optimization error. We find that the error of the automatic estimator is close to the square of the error of the analytic estimator, reflecting a super-efficiency phenomenon. The convergence of the automatic estimator greatly depends on the convergence of the Jacobian of the algorithm. We analyze it for gradient descent and stochastic gradient descent and derive convergence rates for the estimators in these cases. Our analysis is backed by numerical experiments on toy problems and on Wasserstein barycenter computation. Finally, we discuss the computational complexity of these estimators and give practical guidelines to chose between them.
            </div>
        </div>
    </div>
            <div class="talk">
        <div class="talk_item">
            <span class="title">Learning step sizes for unfolded sparse coding </span>
            <span style="width:2em;"></span>

            <a href="/talks/20_01_13_SemOpt_slista.pdf">

                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>


            <br>
            <span class="date">13 Mar 2020,</span>
            <span class="conference">At <i>CIRM - Luminy</i></span>

        </div>

        <input type="checkbox" class="btnCtrl" id="talk17"/>
        <label class="btn display-status" for="talk17">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                Unfolding and learning weights of ISTA using neural networks is a practical way to accelerate the Lasso resolution. However, the reason why learning the weights of such a network would accelerate sparse coding are not clear. In this talk, we look at this problem from the point of view of selecting adapted step sizes for ISTA.
            </div>
            <div class="details">
                Sparse coding is typically solved by iterative optimization techniques, such as the Iterative Shrinkage-Thresholding Algorithm (ISTA). Unfolding and learning weights of ISTA using neural networks is a practical way to accelerate estimation. However, the reason why learning the weights of such a network would accelerate sparse coding are not clear. In this talk, we look at this problem from the point of view of selecting adapted step sizes for ISTA. We show that a simple step size strategy can improve the convergence rate of ISTA by leveraging the sparsity of the iterates. However, it is impractical in most large-scale applications. Therefore, we propose a network architecture where only the step sizes of ISTA are learned. We demonstrate if the learned algorithm converges to the solution of the Lasso, its last layers correspond to ISTA with learned step sizes. Experiments show that learning step sizes can effectively accelerate the convergence when the solutions are sparse enough.
            </div>
        </div>
    </div>

        <div class="project">
    <div class="project_item">
        <span class="title">Joblib 0.14 </span>
        <a href="https://github.com/joblib/joblib"><i class="fa fa-github-alt" aria-hidden="true"></i></a>
        <span class="date">Oct 2019</span>
        <div class="logo">
            <img src="https://joblib.readthedocs.io/en/latest/_static/joblib_logo.svg" style="height: 100%;"/>
        </div>

    </div>

    <input type="checkbox" class="btnCtrl" id="proj5"/>
    <label class="btn display-status" for="proj5">
        <i class="fa fa-plus-circle plus"></i>
        <i class="fa fa-minus-circle minus"></i>
    </label>
    <div class="description">
        <span class="summary">
            Easy and reliable parallel computing with Python functions.
        </span>
        <div class="details">
            Joblib is a set of tools to provide lightweight pipelining in Python. In particular: <ul><li>transparent disk-caching of functions and lazy re-evaluation (memoize pattern)</li><li> easy simple parallel computing</li></ul> Joblib is optimized to be fast and robust on large data in particular and has specific optimizations for numpy arrays. It is BSD-licensed and its documentation can be found at <a href="https://joblib.readthedocs.io/">https://joblib.readthedocs.io/</a>.</br>
</br>
Starting 2017, I have been involved in Joblib development, first to integrate loky as a new multiprocessing backend and them as a regular developer for the library.</br>
</br>
<i>python, multiprocessing, parallel computing</i>
        </div>
    </div>
</div>
    </div><!-- .entry-content -->
</article><!-- #post -->

    </div>
</div><!-- #page -->

<div style="display:none">
</div>

</body>

<script type="text/javascript">
    window.onresize = function(){
        var sidebar = document.getElementById("sidebar");
        var page = document.getElementById("page");
        page.style.setProperty("min-height", sidebar.clientHeight);
    }
    window.onresize()
</script>
</html>
