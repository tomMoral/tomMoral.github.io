
<html lang="en-US">
<!--<![endif]-->
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width" />
<title>Thomas Moreau</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<!--[if lt IE 9]>
<script src="http://dpkingma.com/wordpress/wp-content/themes/twentytwelve/js/html5.js" type="text/javascript"></script>
<![endif]-->
<script src="https://use.fontawesome.com/ae904b9937.js"></script>



<link rel="shortcut icon" href="/images/favicon.png">
<link rel="stylesheet" type="text/css" href="/css/style.css" />
<link rel="stylesheet" href="/css/academicons.css"/>
<script type="text/javascript" src="/javascript/lib.js"></script>
<!-- highlight current page in navigation -->
<script>
$(function(){
  $('a').each(function() {
    if ($(this).prop('href') == window.location.href) {
      $(this).parent().addClass('current-menu-item');
    }
  });
});
</script>

</head>

<body class="home">
<div id="sidebar">
    <header id="masthead" class="site-header" role="banner">
        <hgroup>
            <img class="profile_img" src="https://s.gravatar.com/avatar/1c7c7939d1173cd6f455ce4313e831c5?s=150&r=x" />
            <div class="info">
                <h2 class="site-title"><a href="/about">Thomas Moreau</a></h2>
                <h4 class="site-description">Parietal - Inria Saclay</h4>
                <h5 class="email"> thomas.moreau [AT] inria.fr</h5>
            </div>
        </hgroup>

        <nav id="site-navigation" class="main-navigation" role="navigation">
            <div class="menu-menu-1-container"><ul id="menu-nav" class="menu-nav">
                <li class="nav-menu-item"><a href="/about">About</a></li>
                <li class="nav-menu-item"><a href="/publications">Publications</a></li>
                <li class="nav-menu-item"><a href="/talks">Talks and Videos</a></li>
                <li class="nav-menu-item"><a href="/oss">Open Source Software</a></li>
            </ul></div>
        </nav><!-- #site-navigation -->
        <div id="social-media">
            <a href="https://github.com/tomMoral">
                <i class="fa fa-github fa-3x"></i></a>
            <a href="https://scholar.google.fr/citations?user=HEO_PsAAAAAJ">
                <i class="ai ai-google-scholar ai-3x"></i></a>
            <a href="https://stackoverflow.com/story/tomMoral">
                <i class="fa fa-stack-overflow fa-3x"></i></a>
            <a href="https://linkedin.com/in/tomMoral">
                <i class="fa fa-linkedin fa-3x"></i></a>
        </div>
    </header><!-- #masthead -->
</div>
</div>

<div id="page">
    <div class="page-content">
    <article class="inner-text">
    <header class="entry-header">
        <h1 class="entry-title">About me</h1>
    </header>

    <div class="entry-content">
	<p>
        I am a researcher in the parietal team @ INRIA Saclay, since autumn 2019, working on unsupervised learning for time series and on deep learning methods applied to solving inverse problems.
	</p>

	<p>
        My research interests touch several areas of Machine Learning, Signal Processing and High-Dimensional Statistics. In particular, I have been working on Convolutional Dictionary Learning, studying both their computational aspects and their possible application to pattern analysis. I am also interested in theoretical properties of learned optimization algorithms such as LISTA, in particular for the resolution of inverse problems.
        </p>

	<p>
        I did my PhD under the supervision of Nicolas Vayatis and Laurent Oudre, in the CMLA @ ENS Paris-Saclay. My PhD focuses on convolutional representations and their applications to physiological signals. I continued working on unsupervised learning for time series with application to electrophysiological recordings during a 1,5 year in the Parietal Team. I am also involved in open-source projects such as <span class="highlightcode">joblib</span> or <span class="highlightcode">loky</span>, for parallel scientific computing.
	</p>

	<h3>Latest publication and projects</h3>
        
    <div class="publication">
        <div class="project_item">
            <span class="title">Learning to solve TV regularized problems with unrolled algorithms </span>
            <span style="width:2em;"></span>

            <a href="https://papers.nips.cc/paper/9469-learning-step-sizes-for-unfolded-sparse-coding">
                <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
            </a>


            <a href="https://github.com/hcherkaoui/carpet">
                <i class="fa fa-github-alt" aria-hidden="true"></i>
            </a>


            <br>
            <span class="authors">Cherkaoui Hamza; Sulam Jeremias; Moreau Thomas,</span>
            <span class="date">Dec 2020,</span>

                <span class="conference">In proceedings of <i>Advances in Neural Information Processing System</i></span>


        </div>

        <input type="checkbox" class="btnCtrl" id="pub16"/>
        <label class="btn display-status" for="pub16">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                In this paper, we accelerate such iterative algorithms by unfolding proximal gradient descent solvers in order to learn their parameters for 1D TV regularized problems.  While this could be done using the synthesis formulation, we demonstrate that this leads to slower performances.  The main difficulty in applying such methods in the analysis formulation lies in proposing a way to compute the derivatives through the proximal operator.
            </div>
            <div class="details">
                Total Variation (TV) is a popular regularization strategy that promotes piece-wise constant signals by constraining the`1-norm of the first order derivative of the estimated signal.  The resulting optimization problem is usually solved using iterative algorithms such as proximal gradient descent, primal-dual algorithms or ADMM. However, such methods can require a very large number of iterations to converge to a suitable solution. In this paper, we accelerate such iterative algorithms by unfolding proximal gradient descent solvers in order to learn their parameters for 1D TV regularized problems.  While this could be done using the synthesis formulation, we demonstrate that this leads to slower performances.  The main difficulty in applying such methods in the analysis formulation lies in proposing a way to compute the derivatives through the proximal operator.  As our main contribution, we develop and characterize two approaches to do so, describe their benefits and limitations, and discuss the regime where they can actually improve over iterative procedures. We validate those findings with experiments on synthetic and real data.
            </div>
        </div>
    </div>
            <div class="talk">
        <div class="talk_item">
            <span class="title">Learning to optimize with unrolled algorithms </span>
            <span style="width:2em;"></span>

            <a href="/talks/21_04_15_Monpellier_slista.pdf">

                <img src="/images/slides.png" style="vertical-align:middle;height:1.5em;", alt="slides"/>
            </a>


            <br>
            <span class="date">15 Apr 2021,</span>
            <span class="conference">At <i>ML-MTP seminar - Montpellier</i></span>

        </div>

        <input type="checkbox" class="btnCtrl" id="talk19"/>
        <label class="btn display-status" for="talk19">
            <i class="fa fa-plus-circle plus"></i>
            <i class="fa fa-minus-circle minus"></i>
        </label>
        <div class="description">
            <div class="summary">
                In this talk, I will first review how one can design unrolled algorithms to solve the linear regression with l1 or TV regularization, with a particular focus on the choice of parametrization and loss. Then, I will discuss the reasons why such procedure can lead to better results compared to classical optimization, with a particular focus on the choice of step sizes.
            </div>
            <div class="details">
                When solving multiple optimization problems sharing the same underlying structure, using iterative algorithms designed for worst case scenario can be considered as inefficient. When one aim at having good solution in average, it is possible to improve the performances by learning the weights of a neural networked designed to mimic an unfolded optimization algorithm. However, the reason why learning the weights of such a network would accelerate the problem resolution is not always clear.</br>
In this talk, I will first review how one can design unrolled algorithms to solve the linear regression with l1 or TV regularization, with a particular focus on the choice of parametrization and loss. Then, I will discuss the reasons why such procedure can lead to better results compared to classical optimization, with a particular focus on the choice of step sizes.</br>
</br>
<u>Linked papers:</u><ol><li>Moreau & Bruna. <a href=https://openreview.net/forum?id=SJGPL9Dex>Understanding Neural Sparse Coding with Matrix Factorization</a>. ICLR 2017.</li><li>Ablin, Moreau, Massias & Gramfort. <a href=http://arxiv.org/abs/1905.11071>Learning step sizes for unfolded sparse coding</a>. NeurIPS 2019.</li><li>Cherkaoui, Sulam & Moreau. <a href=https://hal.archives-ouvertes.fr/hal-02954181>Learning to solve TV regularised problems with unrolled algorithms</a>. NeurIPS 2020.</li></ol>
            </div>
        </div>
    </div>

        <div class="project">
    <div class="project_item">
        <span class="title">BenchOpt 1.1 </span>
        <a href="https://github.com/benchopt/benchopt.git"><i class="fa fa-github-alt" aria-hidden="true"></i></a>
        <span class="date">Apr 2021</span>
        <div class="logo"></div>

    </div>

    <input type="checkbox" class="btnCtrl" id="proj6"/>
    <label class="btn display-status" for="proj6">
        <i class="fa fa-plus-circle plus"></i>
        <i class="fa fa-minus-circle minus"></i>
    </label>
    <div class="description">
        <span class="summary">
            Benchmarking tool for optimization.
        </span>
        <div class="details">
            BenchOpt is a package to simplify, make more transparent and more reproducible the comparisons of optimization algorithms.</br>
</br>
BenchOpt is written in Python but it is available with many programming languages. So far it has been tested with Python, R, Julia and compiled binaries written in C/C++ available via a terminal command. If it can be installed via conda it should just work!
        </div>
    </div>
</div>
    </div><!-- .entry-content -->
</article><!-- #post -->

    </div>
</div><!-- #page -->

<div style="display:none">
</div>

</body>

<script type="text/javascript">
    window.onresize = function(){
        var sidebar = document.getElementById("sidebar");
        var page = document.getElementById("page");
        page.style.setProperty("min-height", sidebar.clientHeight);
    }
    window.onresize()
</script>
</html>
